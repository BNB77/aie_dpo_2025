# HW07 – Кластеризация и внутренние метрики качества

Домашнее задание к семинару S07 курса "Искусственный интеллект в образовании".

## Описание

В данной работе реализован полный цикл unsupervised-обучения для задачи кластеризации:

- Применение трех семейств методов кластеризации: **KMeans**, **DBSCAN**, **Agglomerative Clustering**
- Анализ трех синтетических датасетов с различными характеристиками
- Оценка качества кластеризации через внутренние метрики (Silhouette, Davies-Bouldin, Calinski-Harabasz)
- Визуализация результатов через PCA(2D)
- Проверка устойчивости решений

## Структура проекта

```
HW07/
├── HW07.ipynb              # Основной ноутбук с анализом
├── report.md               # Отчет по результатам работы
├── README.md               # Этот файл
├── data/                   # Датасеты
│   ├── S07-hw-dataset-01.csv
│   ├── S07-hw-dataset-02.csv
│   └── S07-hw-dataset-03.csv
└── artifacts/              # Артефакты эксперимента
    ├── metrics_summary.json
    ├── best_configs.json
    ├── labels/             # CSV файлы с метками кластеров
    │   ├── labels_hw07_ds1.csv
    │   ├── labels_hw07_ds2.csv
    │   └── labels_hw07_ds3.csv
    └── figures/            # Визуализации
        ├── ds01_kmeans_metrics_vs_k.png
        ├── ds01_pca_best_solution.png
        ├── ds02_kmeans_metrics_vs_k.png
        ├── ds02_pca_best_solution.png
        ├── ds02_comparison_kmeans_vs_dbscan.png
        ├── ds03_kmeans_metrics_vs_k.png
        ├── ds03_agglomerative_linkage_comparison.png
        ├── ds03_pca_best_solution.png
        └── ds03_stability_check.png
```

## Датасеты

### Dataset 01 (S07-hw-dataset-01.csv)
- **Характеристика:** Числовые признаки в разных шкалах + шумовые признаки
- **Сложность:** Без масштабирования результаты искажаются
- **Применяемые методы:** KMeans, DBSCAN

### Dataset 02 (S07-hw-dataset-02.csv)
- **Характеристика:** Нелинейная структура + выбросы + шумовой признак
- **Сложность:** KMeans проигрывает из-за предположения о сферической форме кластеров
- **Применяемые методы:** KMeans, DBSCAN

### Dataset 03 (S07-hw-dataset-03.csv)
- **Характеристика:** Кластеры разной плотности + фоновый шум
- **Сложность:** Провоцирует ошибки при выборе eps для DBSCAN
- **Применяемые методы:** KMeans, Agglomerative Clustering
- **Дополнительно:** Проверка устойчивости

## Запуск

### Требования

Для работы ноутбука необходимы следующие библиотеки:
- pandas
- numpy
- matplotlib
- scikit-learn
- jupyter

**Установка на Arch Linux:**
```bash
sudo pacman -S python-pandas python-numpy python-matplotlib python-scikit-learn jupyter-notebook
```

Или через виртуальное окружение:
```bash
python -m venv venv
source venv/bin/activate
pip install pandas numpy matplotlib scikit-learn jupyter
```

**Примечание:** Библиотека seaborn указана в импортах ноутбука для стилизации графиков, но может быть опущена - ноутбук будет работать с стандартными стилями matplotlib.

### Выполнение ноутбука

1. Откройте ноутбук `HW07.ipynb` в Jupyter Notebook или JupyterLab
2. Последовательно выполните все ячейки (Kernel → Restart & Run All)
3. После выполнения в папке `artifacts/` будут созданы все необходимые файлы

### Проверка результатов

После выполнения ноутбука проверьте:
- `artifacts/metrics_summary.json` – сводка метрик по всем датасетам
- `artifacts/best_configs.json` – лучшие конфигурации для каждого датасета
- `artifacts/labels/*.csv` – метки кластеров для каждого датасета
- `artifacts/figures/*.png` – визуализации (минимум 9 графиков)
- `report.md` – полный отчет по работе

## Основные результаты

### Методы кластеризации

1. **KMeans**
   - Преимущества: быстр, эффективен для сферических кластеров
   - Недостатки: требует знания k, чувствителен к выбросам, плохо работает с нелинейными структурами

2. **DBSCAN**
   - Преимущества: обнаруживает кластеры произвольной формы, автоматически выделяет выбросы
   - Недостатки: сложен подбор eps, проблемы при разной плотности кластеров

3. **Agglomerative Clustering**
   - Преимущества: иерархическая структура, гибкость через выбор linkage
   - Недостатки: вычислительная сложность O(n³), требует знания количества кластеров

### Внутренние метрики

- **Silhouette Score:** [-1, 1], выше – лучше. Измеряет компактность и разделение кластеров
- **Davies-Bouldin Score:** [0, ∞), ниже – лучше. Отношение внутрикластерного расстояния к межкластерному
- **Calinski-Harabasz Score:** [0, ∞), выше – лучше. Отношение межкластерной дисперсии к внутрикластерной

### Ключевые выводы

1. Препроцессинг (StandardScaler) критически важен для distance-based методов
2. Не существует универсального алгоритма – выбор зависит от структуры данных
3. Внутренние метрики помогают в подборе параметров, но не заменяют содержательный анализ
4. Проверка устойчивости (через ARI) выявляет надежность найденных кластеров
5. PCA визуализация эффективна для интерпретации, но теряет часть информации

## Автор

Выполнено в рамках курса "Искусственный интеллект в образовании" (2025)

## Ссылки

- [Задание HW07](../../aie-course-meta/seminars/S07/S07-homework.md)
- [Шаблон отчета](../../aie-course-meta/seminars/S07/S07-hw-report-template.md)
