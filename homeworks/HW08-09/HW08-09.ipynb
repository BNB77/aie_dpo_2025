{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW08-09: PyTorch MLP — регуляризация и оптимизация обучения\n",
    "\n",
    "**Датасет:** CIFAR-10 (Вариант C)  \n",
    "**Курс:** AIE DPO 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1. Импорты, seed и устройство"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"torch:       {torch.__version__}\")\n",
    "print(f\"torchvision: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Устройство: {device}\")\n",
    "\n",
    "# CIFAR-10: 3 канала × 32×32\n",
    "INPUT_SIZE = 3 * 32 * 32  # 3072\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "ARTIFACTS_DIR = Path('artifacts')\n",
    "FIGURES_DIR = ARTIFACTS_DIR / 'figures'\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "print(f\"artifacts → {ARTIFACTS_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2. Данные и DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация CIFAR-10: mean и std по каждому каналу\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Загрузка через torchvision (скачивается автоматически)\n",
    "full_train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Train (полный): {len(full_train_dataset)}, Test: {len(test_dataset)}\")\n",
    "print(f\"Классы: {full_train_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспроизводимое разбиение train / val = 80/20\n",
    "n_total = len(full_train_dataset)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_val   = n_total - n_train\n",
    "\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset, [n_train, n_val], generator=generator\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          generator=torch.Generator().manual_seed(SEED))\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Sanity check\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"\\nSanity check:\")\n",
    "print(f\"  x.shape = {x.shape}\")\n",
    "print(f\"  y.shape = {y.shape}\")\n",
    "print(f\"  x range = [{x.min():.3f}, {x.max():.3f}]\")\n",
    "print(f\"  y unique = {y.unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3. Модель MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Универсальный MLP: Flatten → [Linear → [BN] → ReLU → [Dropout]] × N → Linear.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=3072, hidden_sizes=None, num_classes=10,\n",
    "                 dropout_p=0.0, use_batchnorm=False):\n",
    "        super().__init__()\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = [512, 256]\n",
    "        layers = [nn.Flatten()]\n",
    "        in_f = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_f, h))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout_p > 0.0:\n",
    "                layers.append(nn.Dropout(dropout_p))\n",
    "            in_f = h\n",
    "        layers.append(nn.Linear(in_f, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Проверка прямого прохода\n",
    "_m = MLP(input_size=INPUT_SIZE)\n",
    "_x = torch.zeros(4, 3, 32, 32)\n",
    "print(f\"MLP forward: {_x.shape} → {_m(_x).shape}\")\n",
    "del _m, _x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Утилиты обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        total   += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss   = criterion(logits, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            correct    += (logits.argmax(1) == y).sum().item()\n",
    "            total      += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Останавливает, если val_accuracy не растёт patience эпох.\"\"\"\n",
    "    def __init__(self, patience=5, min_delta=1e-4):\n",
    "        self.patience, self.min_delta = patience, min_delta\n",
    "        self.best_val_acc = -float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_state = None\n",
    "        self.stopped_epoch = None\n",
    "\n",
    "    def step(self, val_acc, model, epoch):\n",
    "        if val_acc > self.best_val_acc + self.min_delta:\n",
    "            self.best_val_acc = val_acc\n",
    "            self.counter = 0\n",
    "            self.best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        if self.counter >= self.patience:\n",
    "            self.stopped_epoch = epoch\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def restore_best(self, model):\n",
    "        if self.best_state:\n",
    "            model.load_state_dict(self.best_state)\n",
    "\n",
    "\n",
    "def run_experiment(model, train_loader, val_loader, optimizer, criterion, device,\n",
    "                   max_epochs=15, early_stopping=None, tag=''):\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        tl, ta = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        vl, va = evaluate(model, val_loader, criterion, device)\n",
    "        history['train_loss'].append(tl)\n",
    "        history['val_loss'].append(vl)\n",
    "        history['train_acc'].append(ta)\n",
    "        history['val_acc'].append(va)\n",
    "        print(f\"[{tag}] {epoch:02d}/{max_epochs}  \"\n",
    "              f\"train_loss={tl:.4f} train_acc={ta:.4f} | \"\n",
    "              f\"val_loss={vl:.4f} val_acc={va:.4f}\")\n",
    "        if early_stopping and early_stopping.step(va, model, epoch):\n",
    "            print(f\"  >>> EarlyStopping на эпохе {epoch}, \"\n",
    "                  f\"best_val_acc={early_stopping.best_val_acc:.4f}\")\n",
    "            early_stopping.restore_best(model)\n",
    "            break\n",
    "    return history\n",
    "\n",
    "print(\"Утилиты готовы.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.1. Часть A (S08): Регуляризация (E1–E4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion    = nn.CrossEntropyLoss()\n",
    "MAX_EPOCHS_A = 15\n",
    "\n",
    "# ── E1: Base MLP ─────────────────────────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"E1: Base MLP (no Dropout, no BatchNorm)\")\n",
    "print(\"=\" * 60)\n",
    "torch.manual_seed(SEED)\n",
    "model_e1 = MLP(input_size=INPUT_SIZE, dropout_p=0.0, use_batchnorm=False).to(device)\n",
    "history_e1 = run_experiment(\n",
    "    model_e1, train_loader, val_loader,\n",
    "    optim.Adam(model_e1.parameters(), lr=1e-3),\n",
    "    criterion, device, max_epochs=MAX_EPOCHS_A, tag='E1')\n",
    "best_val_acc_e1  = max(history_e1['val_acc'])\n",
    "best_val_loss_e1 = history_e1['val_loss'][history_e1['val_acc'].index(best_val_acc_e1)]\n",
    "print(f\"\\n[E1] best val_acc = {best_val_acc_e1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── E2: Dropout ───────────────────────────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"E2: MLP + Dropout(p=0.3)\")\n",
    "print(\"=\" * 60)\n",
    "torch.manual_seed(SEED)\n",
    "model_e2 = MLP(input_size=INPUT_SIZE, dropout_p=0.3, use_batchnorm=False).to(device)\n",
    "history_e2 = run_experiment(\n",
    "    model_e2, train_loader, val_loader,\n",
    "    optim.Adam(model_e2.parameters(), lr=1e-3),\n",
    "    criterion, device, max_epochs=MAX_EPOCHS_A, tag='E2')\n",
    "best_val_acc_e2  = max(history_e2['val_acc'])\n",
    "best_val_loss_e2 = history_e2['val_loss'][history_e2['val_acc'].index(best_val_acc_e2)]\n",
    "print(f\"\\n[E2] best val_acc = {best_val_acc_e2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── E3: BatchNorm ─────────────────────────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"E3: MLP + BatchNorm\")\n",
    "print(\"=\" * 60)\n",
    "torch.manual_seed(SEED)\n",
    "model_e3 = MLP(input_size=INPUT_SIZE, dropout_p=0.0, use_batchnorm=True).to(device)\n",
    "history_e3 = run_experiment(\n",
    "    model_e3, train_loader, val_loader,\n",
    "    optim.Adam(model_e3.parameters(), lr=1e-3),\n",
    "    criterion, device, max_epochs=MAX_EPOCHS_A, tag='E3')\n",
    "best_val_acc_e3  = max(history_e3['val_acc'])\n",
    "best_val_loss_e3 = history_e3['val_loss'][history_e3['val_acc'].index(best_val_acc_e3)]\n",
    "print(f\"\\n[E3] best val_acc = {best_val_acc_e3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── E4: лучший из E2/E3 + EarlyStopping ─────────────────────────────────────\n",
    "if best_val_acc_e3 >= best_val_acc_e2:\n",
    "    best_base_tag = 'E3';  e4_dropout_p = 0.0;  e4_batchnorm = True\n",
    "else:\n",
    "    best_base_tag = 'E2';  e4_dropout_p = 0.3;  e4_batchnorm = False\n",
    "\n",
    "print(f\"Лучший между E2/E3: {best_base_tag}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"E4: {best_base_tag} + EarlyStopping(patience=5)\")\n",
    "print(\"=\" * 60)\n",
    "torch.manual_seed(SEED)\n",
    "model_e4 = MLP(input_size=INPUT_SIZE, dropout_p=e4_dropout_p,\n",
    "               use_batchnorm=e4_batchnorm).to(device)\n",
    "es = EarlyStopping(patience=5)\n",
    "history_e4 = run_experiment(\n",
    "    model_e4, train_loader, val_loader,\n",
    "    optim.Adam(model_e4.parameters(), lr=1e-3),\n",
    "    criterion, device, max_epochs=30, early_stopping=es, tag='E4')\n",
    "\n",
    "best_val_acc_e4  = es.best_val_acc\n",
    "best_val_loss_e4 = history_e4['val_loss'][history_e4['val_acc'].index(max(history_e4['val_acc']))]\n",
    "epochs_e4        = len(history_e4['val_acc'])\n",
    "print(f\"\\n[E4] best val_acc = {best_val_acc_e4:.4f}, остановлен на эпохе {es.stopped_epoch}\")\n",
    "\n",
    "torch.save(model_e4.state_dict(), str(ARTIFACTS_DIR / 'best_model.pt'))\n",
    "print(\"Сохранено: artifacts/best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.2. Часть B (S09): LR, оптимизаторы, weight decay (O1–O3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── O1: lr слишком большой ───────────────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"O1: Adam, lr=0.1 (слишком большой)\")\n",
    "print(\"=\" * 60)\n",
    "torch.manual_seed(SEED)\n",
    "model_o1 = MLP(input_size=INPUT_SIZE, dropout_p=e4_dropout_p,\n",
    "               use_batchnorm=e4_batchnorm).to(device)\n",
    "history_o1 = run_experiment(\n",
    "    model_o1, train_loader, val_loader,\n",
    "    optim.Adam(model_o1.parameters(), lr=0.1),\n",
    "    criterion, device, max_epochs=8, tag='O1')\n",
    "best_val_acc_o1  = max(history_o1['val_acc'])\n",
    "best_val_loss_o1 = min(history_o1['val_loss'])\n",
    "print(f\"\\n[O1] best val_acc = {best_val_acc_o1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── O2: lr слишком маленький ─────────────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"O2: Adam, lr=1e-5 (слишком маленький)\")\n",
    "print(\"=\" * 60)\n",
    "torch.manual_seed(SEED)\n",
    "model_o2 = MLP(input_size=INPUT_SIZE, dropout_p=e4_dropout_p,\n",
    "               use_batchnorm=e4_batchnorm).to(device)\n",
    "history_o2 = run_experiment(\n",
    "    model_o2, train_loader, val_loader,\n",
    "    optim.Adam(model_o2.parameters(), lr=1e-5),\n",
    "    criterion, device, max_epochs=8, tag='O2')\n",
    "best_val_acc_o2  = max(history_o2['val_acc'])\n",
    "best_val_loss_o2 = min(history_o2['val_loss'])\n",
    "print(f\"\\n[O2] best val_acc = {best_val_acc_o2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── O3: SGD + momentum + weight_decay ────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"O3: SGD, momentum=0.9, weight_decay=1e-4, lr=0.01\")\n",
    "print(\"=\" * 60)\n",
    "torch.manual_seed(SEED)\n",
    "model_o3 = MLP(input_size=INPUT_SIZE, dropout_p=e4_dropout_p,\n",
    "               use_batchnorm=e4_batchnorm).to(device)\n",
    "history_o3 = run_experiment(\n",
    "    model_o3, train_loader, val_loader,\n",
    "    optim.SGD(model_o3.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4),\n",
    "    criterion, device, max_epochs=15, tag='O3')\n",
    "best_val_acc_o3  = max(history_o3['val_acc'])\n",
    "best_val_loss_o3 = history_o3['val_loss'][history_o3['val_acc'].index(best_val_acc_o3)]\n",
    "epochs_o3        = len(history_o3['val_acc'])\n",
    "print(f\"\\n[O3] best val_acc = {best_val_acc_o3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Артефакты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── runs.csv ──────────────────────────────────────────────────────────────────\n",
    "DATASET = 'CIFAR10'\n",
    "bn_tag  = '+BN' if e4_batchnorm else 'Dropout(0.3)'\n",
    "\n",
    "rows = [\n",
    "    dict(experiment_id='E1', dataset=DATASET, seed=SEED,\n",
    "         model_summary='512-256, ReLU, no-Dropout, no-BN',\n",
    "         optimizer='Adam', lr=1e-3, momentum='', weight_decay=0,\n",
    "         epochs_trained=len(history_e1['val_acc']),\n",
    "         best_val_accuracy=round(best_val_acc_e1, 4),\n",
    "         best_val_loss=round(best_val_loss_e1, 4)),\n",
    "    dict(experiment_id='E2', dataset=DATASET, seed=SEED,\n",
    "         model_summary='512-256, ReLU, Dropout(0.3), no-BN',\n",
    "         optimizer='Adam', lr=1e-3, momentum='', weight_decay=0,\n",
    "         epochs_trained=len(history_e2['val_acc']),\n",
    "         best_val_accuracy=round(best_val_acc_e2, 4),\n",
    "         best_val_loss=round(best_val_loss_e2, 4)),\n",
    "    dict(experiment_id='E3', dataset=DATASET, seed=SEED,\n",
    "         model_summary='512-256, ReLU, no-Dropout, BatchNorm',\n",
    "         optimizer='Adam', lr=1e-3, momentum='', weight_decay=0,\n",
    "         epochs_trained=len(history_e3['val_acc']),\n",
    "         best_val_accuracy=round(best_val_acc_e3, 4),\n",
    "         best_val_loss=round(best_val_loss_e3, 4)),\n",
    "    dict(experiment_id='E4', dataset=DATASET, seed=SEED,\n",
    "         model_summary=f'512-256, ReLU, {bn_tag}, EarlyStopping(p=5)',\n",
    "         optimizer='Adam', lr=1e-3, momentum='', weight_decay=0,\n",
    "         epochs_trained=epochs_e4,\n",
    "         best_val_accuracy=round(best_val_acc_e4, 4),\n",
    "         best_val_loss=round(best_val_loss_e4, 4)),\n",
    "    dict(experiment_id='O1', dataset=DATASET, seed=SEED,\n",
    "         model_summary=f'512-256, ReLU, {bn_tag}',\n",
    "         optimizer='Adam', lr=0.1, momentum='', weight_decay=0,\n",
    "         epochs_trained=len(history_o1['val_acc']),\n",
    "         best_val_accuracy=round(best_val_acc_o1, 4),\n",
    "         best_val_loss=round(best_val_loss_o1, 4)),\n",
    "    dict(experiment_id='O2', dataset=DATASET, seed=SEED,\n",
    "         model_summary=f'512-256, ReLU, {bn_tag}',\n",
    "         optimizer='Adam', lr=1e-5, momentum='', weight_decay=0,\n",
    "         epochs_trained=len(history_o2['val_acc']),\n",
    "         best_val_accuracy=round(best_val_acc_o2, 4),\n",
    "         best_val_loss=round(best_val_loss_o2, 4)),\n",
    "    dict(experiment_id='O3', dataset=DATASET, seed=SEED,\n",
    "         model_summary=f'512-256, ReLU, {bn_tag}',\n",
    "         optimizer='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4,\n",
    "         epochs_trained=epochs_o3,\n",
    "         best_val_accuracy=round(best_val_acc_o3, 4),\n",
    "         best_val_loss=round(best_val_loss_o3, 4)),\n",
    "]\n",
    "\n",
    "fieldnames = ['experiment_id','dataset','seed','model_summary','optimizer',\n",
    "              'lr','momentum','weight_decay','epochs_trained',\n",
    "              'best_val_accuracy','best_val_loss']\n",
    "with open(ARTIFACTS_DIR / 'runs.csv', 'w', newline='') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "\n",
    "print(\"Сохранено: artifacts/runs.csv\")\n",
    "print(f\"\\n{'ID':<4} {'opt':<5} {'lr':<8} {'wd':<8} {'ep':<4} {'val_acc':<9} val_loss\")\n",
    "print('-' * 50)\n",
    "for r in rows:\n",
    "    print(f\"{r['experiment_id']:<4} {r['optimizer']:<5} {r['lr']:<8} \"\n",
    "          f\"{str(r['weight_decay']):<8} {r['epochs_trained']:<4} \"\n",
    "          f\"{r['best_val_accuracy']:<9} {r['best_val_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── best_config.json ──────────────────────────────────────────────────────────\n",
    "best_config = {\n",
    "    'dataset': DATASET, 'seed': SEED,\n",
    "    'model': {\n",
    "        'class': 'MLP', 'input_size': INPUT_SIZE,\n",
    "        'hidden_sizes': [512, 256], 'num_classes': NUM_CLASSES,\n",
    "        'activation': 'ReLU', 'dropout_p': e4_dropout_p,\n",
    "        'use_batchnorm': e4_batchnorm,\n",
    "    },\n",
    "    'training': {\n",
    "        'optimizer': 'Adam', 'lr': 1e-3, 'weight_decay': 0,\n",
    "        'batch_size': BATCH_SIZE, 'max_epochs': 30,\n",
    "        'early_stopping_patience': 5, 'epochs_trained': epochs_e4,\n",
    "    },\n",
    "    'results': {\n",
    "        'best_val_accuracy': round(best_val_acc_e4, 4),\n",
    "        'best_val_loss':     round(best_val_loss_e4, 4),\n",
    "    }\n",
    "}\n",
    "with open(ARTIFACTS_DIR / 'best_config.json', 'w') as f:\n",
    "    json.dump(best_config, f, indent=2)\n",
    "print(\"Сохранено: artifacts/best_config.json\")\n",
    "print(json.dumps(best_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Графики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── curves_best.png ───────────────────────────────────────────────────────────\n",
    "ep4 = range(1, len(history_e4['train_loss']) + 1)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(ep4, history_e4['train_loss'], 'o-', ms=3, label='train')\n",
    "ax1.plot(ep4, history_e4['val_loss'],   's-', ms=3, label='val')\n",
    "ax1.set(xlabel='Epoch', ylabel='Loss',\n",
    "        title=f'E4 ({best_base_tag}+EarlyStopping): Loss')\n",
    "ax1.legend(); ax1.grid(alpha=0.3)\n",
    "ax2.plot(ep4, history_e4['train_acc'], 'o-', ms=3, label='train')\n",
    "ax2.plot(ep4, history_e4['val_acc'],   's-', ms=3, label='val')\n",
    "ax2.set(xlabel='Epoch', ylabel='Accuracy',\n",
    "        title=f'E4 ({best_base_tag}+EarlyStopping): Accuracy')\n",
    "ax2.legend(); ax2.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(FIGURES_DIR / 'curves_best.png'), dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Сохранено: artifacts/figures/curves_best.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── curves_lr_extremes.png ────────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for ax, hist, title in [\n",
    "    (axes[0], history_o1, 'O1: Adam lr=0.1 (слишком большой)'),\n",
    "    (axes[1], history_o2, 'O2: Adam lr=1e-5 (слишком маленький)'),\n",
    "]:\n",
    "    ep = range(1, len(hist['train_loss']) + 1)\n",
    "    ax.plot(ep, hist['train_loss'], 'o-', ms=4, label='train')\n",
    "    ax.plot(ep, hist['val_loss'],   's-', ms=4, label='val')\n",
    "    ax.set(xlabel='Epoch', ylabel='Loss', title=title)\n",
    "    ax.legend(); ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(FIGURES_DIR / 'curves_lr_extremes.png'), dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Сохранено: artifacts/figures/curves_lr_extremes.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Финальная оценка на test (один раз)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем сохранённые веса и оцениваем на test — ОДИН РАЗ\n",
    "model_e4.load_state_dict(\n",
    "    torch.load(str(ARTIFACTS_DIR / 'best_model.pt'), map_location=device)\n",
    ")\n",
    "test_loss, test_acc = evaluate(model_e4, test_loader, criterion, device)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Финальная оценка E4 на test\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"test_loss     = {test_loss:.4f}\")\n",
    "print(f\"test_accuracy = {test_acc:.4f}\")\n",
    "print(\"\\nОценка test-выборки выполнена ОДИН РАЗ.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
